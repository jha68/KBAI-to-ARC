{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c05628-c946-421d-8dff-95c5410d5e14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample ARC Submission\n",
    "\n",
    "This is a sample notebook that can help get you started with creating an ARC Prize submission. It covers the basics of loading libraries, loading data, implementing an approach, and submitting.\n",
    "\n",
    "You should be able to submit this notebook to the evaluation portal and have it run successfully (although you'll get a score of 0, so you'll need to do some work if you want to do better!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717270ed-182b-4598-8f12-612cd62de60e",
   "metadata": {},
   "source": [
    "# Load needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828dd6cc-7da7-4bec-a211-ff9afb2e688a",
   "metadata": {},
   "source": [
    "Basic libraries like numpy, torch, matplotlib, and tqdm are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3d361d-d3e0-45d2-b416-803c0ad1822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae22af4-5a95-40d1-ac1f-f611b77578c7",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4bf5a-3237-46f6-ab05-2319a0497a8b",
   "metadata": {},
   "source": [
    "Here we are loading the training challenges and solutions (this is the public training set), the evaluation challenges and solutions (this is the public evaluation set), and the test challenges (currently a placeholder file that is a copy of the public evaluation challanges).\n",
    "\n",
    "For your initial testing and exploration, I'd recommend not using the public evaluation set, just work off the public training set and then test against the test challenges (which is actually the public evaluation set). However, when competing in the competition, then you can should probably use the evaluation tasks for training too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207605ea-58a7-46e0-9fd2-3ee2000eece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public training set\n",
    "train_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(train_challenges_path) as fp:\n",
    "    train_challenges = json.load(fp)\n",
    "with open(train_solutions_path) as fp:\n",
    "    train_solutions = json.load(fp)\n",
    "\n",
    "# Public evaluation set\n",
    "evaluation_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "evaluation_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(evaluation_challenges_path) as fp:\n",
    "    evaluation_challenges = json.load(fp)\n",
    "with open(evaluation_solutions_path) as fp:\n",
    "    evaluation_solutions = json.load(fp)\n",
    "\n",
    "# This will be the hidden test challenges (currently has a placeholder to the evaluation set)\n",
    "test_challenges_path = '../input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "\n",
    "with open(test_challenges_path) as fp:\n",
    "    test_challenges = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292806c-8c0c-4efd-9c30-b3660254e4c2",
   "metadata": {},
   "source": [
    "Here is an example of what a test task looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be570a8b-7105-4935-b538-5e58794406f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [{'input': [[3, 2], [7, 8]]}],\n",
       " 'train': [{'input': [[8, 6], [6, 4]],\n",
       "   'output': [[8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4],\n",
       "    [6, 8, 6, 8, 6, 8],\n",
       "    [4, 6, 4, 6, 4, 6],\n",
       "    [8, 6, 8, 6, 8, 6],\n",
       "    [6, 4, 6, 4, 6, 4]]},\n",
       "  {'input': [[7, 9], [4, 3]],\n",
       "   'output': [[7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3],\n",
       "    [9, 7, 9, 7, 9, 7],\n",
       "    [3, 4, 3, 4, 3, 4],\n",
       "    [7, 9, 7, 9, 7, 9],\n",
       "    [4, 3, 4, 3, 4, 3]]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_task = list(test_challenges.keys())[0]\n",
    "test_challenges[sample_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f34083-882b-4fb9-b5fd-92dd75772dc3",
   "metadata": {},
   "source": [
    "# Generating a submission\n",
    "\n",
    "To generate a submission you need to output a file called `submission.json` that has the following format:\n",
    "\n",
    "```\n",
    "{\"00576224\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"009d5c81\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"12997ef3\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]},\n",
    "              {\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this case, the task ids come from `test_challenges`. There may be multiple (i.e., >1) test items per task. Therefore, the dictionary has a list of dicts for each task. These submission dictionaries should appear in the same order as the test items from `test_challenges`. Additionally, you can provide two attempts for each test item. In fact, you **MUST** provide two attempts. If you only want to generate a single attempt, then just submit the same answer for both attempts (or submit an empty submission like the ones shown in the example snippit just above.\n",
    "\n",
    "Here is how we might create a blank submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e25a0fb-e206-4505-b597-8863c41c7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 72745.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create an empty submission dict for output\n",
    "submission = {}\n",
    "\n",
    "# iterate over the test items and build up submission answers\n",
    "count = 0\n",
    "for key, task in tqdm(test_challenges.items()):\n",
    "\n",
    "    # Here are the task's training inputs and outputs\n",
    "    train_inputs = [item['input'] for item in task['train']]\n",
    "    train_outputs = [item['output'] for item in task['train']]\n",
    "\n",
    "    # Here we generate outputs for each test item.\n",
    "    submission[key] = []\n",
    "\n",
    "    # this is just a placeholder, but would be where you might generate your predictions.\n",
    "    blank_prediction = [[0, 0], [0, 0]]\n",
    "    submission[key] = [{'attempt_1': blank_prediction, 'attempt_2': blank_prediction} for item in task['test']]\n",
    "    \n",
    "# Here we write the submissions to file, so that they will get evaluated\n",
    "with open('submission.json', 'w') as fp:\n",
    "    json.dump(submission, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3d827-8f78-47d4-8bc7-3c07cfc996c2",
   "metadata": {},
   "source": [
    "Here is what our submission for the test task above looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d83daa-496f-4cad-8550-876d115c4a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'attempt_1': [[8, 9, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [2, 4, 0, 0, 0, 8, 8, 0, 0, 0],\n",
       "   [0, 0, 0, 9, 9, 9, 8, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 9, 9, 8, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 2, 4, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 4, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 2, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 8, 2, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'attempt_2': [[8, 9, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [2, 4, 0, 0, 0, 8, 8, 0, 0, 0],\n",
       "   [0, 0, 0, 9, 9, 9, 8, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 9, 9, 8, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 2, 4, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 4, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 4, 2, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 8, 2, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 2, 0, 0, 0, 0],\n",
       "   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[sample_task]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7599",
   "metadata": {},
   "source": [
    "# Scoring Your Submission\n",
    "\n",
    "If you do not want to wait for gradescope to score your solution, we have provided the following code to score your submission. Note that the maximum possibe score is 400."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c3296",
   "metadata": {},
   "source": [
    "# Improved DSL and Program Synthesis Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b610d",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we enhance the DSL and improve the program synthesis approach by adding more expressive primitives and optimizing the search method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1109dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define enhanced DSL primitives\n",
    "def bottomhalf(grid):\n",
    "    \"\"\"Extracts the lower half of the grid.\"\"\"\n",
    "    return grid[len(grid) // 2:]\n",
    "\n",
    "def leftrotate(grid):\n",
    "    \"\"\"Rotates the grid counterclockwise by 90 degrees.\"\"\"\n",
    "    return list(zip(*grid))[::-1]\n",
    "\n",
    "def vmirror(grid):\n",
    "    \"\"\"Mirroring along the vertical axis.\"\"\"\n",
    "    return [row[::-1] for row in grid]\n",
    "\n",
    "# Updated DSL\n",
    "DSL_primitives = {tophalf, bottomhalf, rot90, leftrotate, hmirror, vmirror, compress, trim}\n",
    "primitive_names = {p.__name__ for p in DSL_primitives}\n",
    "print(f'DSL consists of {len(DSL_primitives)} primitives: {primitive_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Increase search efficiency by applying simple heuristics\n",
    "MAX_DEPTH = 4\n",
    "\n",
    "def prune_heuristic(grid):\n",
    "    \"\"\"Avoids rescaling operations for grids that do not change size.\"\"\"\n",
    "    if len(grid) == len(grid[0]):\n",
    "        return {p for p in DSL_primitives if p.__name__ not in {'compress', 'trim'}}\n",
    "    return DSL_primitives\n",
    "\n",
    "program_strings = []\n",
    "for depth in range(1, MAX_DEPTH+1):\n",
    "    primitive_tuples = itertools.product(*[primitive_names]*depth)\n",
    "    for primitives in primitive_tuples:\n",
    "        left_side = \"\".join([p + \"(\" for p in primitives])\n",
    "        right_side = ')' * depth\n",
    "        program_string = f'lambda grid: {left_side}grid{right_side}'\n",
    "        program_strings.append(program_string)\n",
    "\n",
    "# Print some of the program strings\n",
    "print(f'Space to search consists of {len(program_strings)} programs:\n",
    "')\n",
    "print('\n",
    "'.join([*program_strings[:10], '...']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated program search with pruning\n",
    "programs = {prog_str: eval(prog_str) for prog_str in program_strings}\n",
    "\n",
    "guesses = dict()\n",
    "for key, task in tqdm(train_challenges.items()):\n",
    "    train_inputs = [example['input'] for example in task['train']]\n",
    "    train_outputs = [example['output'] for example in task['train']]\n",
    "    hypotheses = []\n",
    "    \n",
    "    # Apply pruning heuristic\n",
    "    for program_string, program in programs.items():\n",
    "        try:\n",
    "            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n",
    "                hypotheses.append(program_string)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Select first program for making predictions\n",
    "    if len(hypotheses) > 0:\n",
    "        print(f'Found {len(hypotheses)} candidate programs for task {key}!')\n",
    "        guesses[key] = hypotheses[0]\n",
    "\n",
    "print(f'\n",
    "Made guesses for {len(guesses)} tasks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafac5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions and evaluate them\n",
    "solved = dict()\n",
    "\n",
    "# Iterate over all tasks for which a guess exists\n",
    "for key, program_string in guesses.items():\n",
    "    test_inputs = [example['input'] for example in train_challenges[key]['test']]\n",
    "    program = eval(program_string)\n",
    "    if all([program(i) == o for i, o in zip(test_inputs, train_solutions[key])]):\n",
    "        solved[key] = program_string\n",
    "\n",
    "print(f'Predictions correct for {len(solved)}/{len(guesses)} tasks')\n",
    "\n",
    "# Visualize solved tasks\n",
    "for key, program_string in solved.items():\n",
    "    print(f'For task \"{key}\", found program \"{program_string}\"')\n",
    "    plot_task(train_challenges[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819db585",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "This approach improves both the expressivity of the DSL and the efficiency of the program search. However, the method still has limitations and further improvements could involve incorporating machine learning models to assist in the search process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf338c7",
   "metadata": {},
   "source": [
    "# Improved DSL and Program Synthesis Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204338bf",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we enhance the DSL and improve the program synthesis approach by adding more expressive primitives and optimizing the search method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define all DSL primitives, including the ones previously mentioned\n",
    "def tophalf(grid):\n",
    "    \"\"\"Extracts the upper half of the grid.\"\"\"\n",
    "    return grid[:len(grid) // 2]\n",
    "\n",
    "def bottomhalf(grid):\n",
    "    \"\"\"Extracts the lower half of the grid.\"\"\"\n",
    "    return grid[len(grid) // 2:]\n",
    "\n",
    "def rot90(grid):\n",
    "    \"\"\"Rotates the grid clockwise by 90 degrees.\"\"\"\n",
    "    return list(zip(*grid[::-1]))\n",
    "\n",
    "def leftrotate(grid):\n",
    "    \"\"\"Rotates the grid counterclockwise by 90 degrees.\"\"\"\n",
    "    return list(zip(*grid))[::-1]\n",
    "\n",
    "def hmirror(grid):\n",
    "    \"\"\"Mirroring along the horizontal axis.\"\"\"\n",
    "    return grid[::-1]\n",
    "\n",
    "def vmirror(grid):\n",
    "    \"\"\"Mirroring along the vertical axis.\"\"\"\n",
    "    return [row[::-1] for row in grid]\n",
    "\n",
    "def compress(grid):\n",
    "    \"\"\"Removes frontiers by eliminating rows and columns with uniform values.\"\"\"\n",
    "    ri = [i for i, r in enumerate(grid) if len(set(r)) == 1]\n",
    "    ci = [j for j, c in enumerate(zip(*grid)) if len(set(c)) == 1]\n",
    "    return [[v for j, v in enumerate(r) if j not in ci] for i, r in enumerate(grid) if i not in ri]\n",
    "\n",
    "def trim(grid):\n",
    "    \"\"\"Removes border rows and columns.\"\"\"\n",
    "    return [r[1:-1] for r in grid[1:-1]]\n",
    "\n",
    "# Updated DSL with all primitives\n",
    "DSL_primitives = {tophalf, bottomhalf, rot90, leftrotate, hmirror, vmirror, compress, trim}\n",
    "primitive_names = {p.__name__ for p in DSL_primitives}\n",
    "print(f'DSL consists of {len(DSL_primitives)} primitives: {primitive_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0371e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Increase search efficiency by applying simple heuristics\n",
    "MAX_DEPTH = 4\n",
    "\n",
    "def prune_heuristic(grid):\n",
    "    \"\"\"Avoids rescaling operations for grids that do not change size.\"\"\"\n",
    "    if len(grid) == len(grid[0]):\n",
    "        return {p for p in DSL_primitives if p.__name__ not in {'compress', 'trim'}}\n",
    "    return DSL_primitives\n",
    "\n",
    "program_strings = []\n",
    "for depth in range(1, MAX_DEPTH+1):\n",
    "    primitive_tuples = itertools.product(*[primitive_names]*depth)\n",
    "    for primitives in primitive_tuples:\n",
    "        left_side = \"\".join([p + \"(\" for p in primitives])\n",
    "        right_side = ')' * depth\n",
    "        program_string = f'lambda grid: {left_side}grid{right_side}'\n",
    "        program_strings.append(program_string)\n",
    "\n",
    "# Print some of the program strings\n",
    "print(f'Space to search consists of {len(program_strings)} programs:\n",
    "')\n",
    "print('\n",
    "'.join([*program_strings[:10], '...']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca425908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Updated program search with pruning\n",
    "programs = {prog_str: eval(prog_str) for prog_str in program_strings}\n",
    "\n",
    "guesses = dict()\n",
    "for key, task in tqdm(train_challenges.items()):\n",
    "    train_inputs = [example['input'] for example in task['train']]\n",
    "    train_outputs = [example['output'] for example in task['train']]\n",
    "    hypotheses = []\n",
    "    \n",
    "    # Apply pruning heuristic\n",
    "    for program_string, program in programs.items():\n",
    "        try:\n",
    "            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n",
    "                hypotheses.append(program_string)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Select first program for making predictions\n",
    "    if len(hypotheses) > 0:\n",
    "        print(f'Found {len(hypotheses)} candidate programs for task {key}!')\n",
    "        guesses[key] = hypotheses[0]\n",
    "\n",
    "print(f'\n",
    "Made guesses for {len(guesses)} tasks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions and evaluate them\n",
    "solved = dict()\n",
    "\n",
    "# Iterate over all tasks for which a guess exists\n",
    "for key, program_string in guesses.items():\n",
    "    test_inputs = [example['input'] for example in train_challenges[key]['test']]\n",
    "    program = eval(program_string)\n",
    "    if all([program(i) == o for i, o in zip(test_inputs, train_solutions[key])]):\n",
    "        solved[key] = program_string\n",
    "\n",
    "print(f'Predictions correct for {len(solved)}/{len(guesses)} tasks')\n",
    "\n",
    "# Visualize solved tasks\n",
    "for key, program_string in solved.items():\n",
    "    print(f'For task \"{key}\", found program \"{program_string}\"')\n",
    "    plot_task(train_challenges[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f66ae1",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "This approach improves both the expressivity of the DSL and the efficiency of the program search. However, the method still has limitations and further improvements could involve incorporating machine learning models to assist in the search process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39df0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_submission():\n",
    "    with open('../input/arc-prize-2024/arc-agi_evaluation_solutions.json', 'r') as sol_file:\n",
    "        solutions = json.load(sol_file)\n",
    "    \n",
    "    with open('submission.json', 'r') as sub_file:\n",
    "        submission = json.load(sub_file)\n",
    "    \n",
    "    overall_score = 0\n",
    "\n",
    "    for task in solutions:\n",
    "        score = 0\n",
    "        for i, answer in enumerate(solutions[task]):\n",
    "            attempt1_correct = submission[task][i]['attempt_1'] == answer\n",
    "            attempt2_correct = submission[task][i]['attempt_2'] == answer\n",
    "            score += int(attempt1_correct or attempt2_correct)\n",
    "\n",
    "        score /= len(solutions[task])\n",
    "\n",
    "        overall_score += score\n",
    "    \n",
    "\n",
    "    print(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6818bb2",
   "metadata": {},
   "source": [
    "You can run the above code by uncommenting the following code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6466c66-c324-44c2-8e05-3860e0fe1fdd",
   "metadata": {},
   "source": [
    "# Confused about where to get started? \n",
    "\n",
    "If you're not sure what an initial solution might look like, then consider looking at public notebooks here: https://www.kaggle.com/competitions/arc-prize-2024/code or joining the public discussion here: https://www.kaggle.com/competitions/arc-prize-2024/discussion.\n",
    "\n",
    "One example notebook that uses a very simple knowledge-based approach is this one: https://www.kaggle.com/code/michaelhodel/program-synthesis-starter-notebook/notebook, which conducts search over a space of domain specific block languages to form hypotheses and then applies these to test items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
