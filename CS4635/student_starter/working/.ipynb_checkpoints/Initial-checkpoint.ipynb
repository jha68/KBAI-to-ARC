{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c05628-c946-421d-8dff-95c5410d5e14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample ARC Submission\n",
    "\n",
    "This is a sample notebook that can help get you started with creating an ARC Prize submission. It covers the basics of loading libraries, loading data, implementing an approach, and submitting.\n",
    "\n",
    "You should be able to submit this notebook to the evaluation portal and have it run successfully (although you'll get a score of 0, so you'll need to do some work if you want to do better!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717270ed-182b-4598-8f12-612cd62de60e",
   "metadata": {},
   "source": [
    "# Load needed libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828dd6cc-7da7-4bec-a211-ff9afb2e688a",
   "metadata": {},
   "source": [
    "Basic libraries like numpy, torch, matplotlib, and tqdm are already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca3d361d-d3e0-45d2-b416-803c0ad1822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import itertools\n",
    "from random import sample\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap, Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae22af4-5a95-40d1-ac1f-f611b77578c7",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4bf5a-3237-46f6-ab05-2319a0497a8b",
   "metadata": {},
   "source": [
    "Here we are loading the training challenges and solutions (this is the public training set), the evaluation challenges and solutions (this is the public evaluation set), and the test challenges (currently a placeholder file that is a copy of the public evaluation challanges).\n",
    "\n",
    "For your initial testing and exploration, I'd recommend not using the public evaluation set, just work off the public training set and then test against the test challenges (which is actually the public evaluation set). However, when competing in the competition, then you can should probably use the evaluation tasks for training too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "207605ea-58a7-46e0-9fd2-3ee2000eece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public training set\n",
    "train_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "train_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "with open(train_challenges_path) as fp:\n",
    "    train_challenges = json.load(fp)\n",
    "with open(train_solutions_path) as fp:\n",
    "    train_solutions = json.load(fp)\n",
    "\n",
    "# # Public evaluation set\n",
    "# evaluation_challenges_path = '../input/arc-prize-2024/arc-agi_training_challenges.json'\n",
    "# evaluation_solutions_path = '../input/arc-prize-2024/arc-agi_training_solutions.json'\n",
    "\n",
    "# with open(evaluation_challenges_path) as fp:\n",
    "#     evaluation_challenges = json.load(fp)\n",
    "# with open(evaluation_solutions_path) as fp:\n",
    "#     evaluation_solutions = json.load(fp)\n",
    "\n",
    "# This will be the hidden test challenges (currently has a placeholder to the evaluation set)\n",
    "test_challenges_path = '../input/arc-prize-2024/arc-agi_test_challenges.json'\n",
    "\n",
    "with open(test_challenges_path) as fp:\n",
    "    test_challenges = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be570a8b-7105-4935-b538-5e58794406f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSL consists of 11 primitives: {'rotate270', 'tophalf', 'invert_colors', 'bottomhalf', 'hmirror', 'trim', 'rot90', 'compress', 'vmirror', 'leftrotate', 'rotate180'}\n",
      "Space to search consists of 11 programs:\n",
      "\n",
      "lambda grid: rotate270(grid)\n",
      "lambda grid: tophalf(grid)\n",
      "lambda grid: invert_colors(grid)\n",
      "lambda grid: bottomhalf(grid)\n",
      "lambda grid: hmirror(grid)\n",
      "lambda grid: trim(grid)\n",
      "lambda grid: rot90(grid)\n",
      "lambda grid: compress(grid)\n",
      "lambda grid: vmirror(grid)\n",
      "lambda grid: leftrotate(grid)\n",
      "...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 400/400 [00:00<00:00, 10513.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 candidate programs for task 1cf80156!\n",
      "found 1 candidate programs for task 67a3c6ac!\n",
      "found 1 candidate programs for task 68b16354!\n",
      "\n",
      "Made guesses for 3 tasks\n",
      "Predictions correct for 3/3 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# defining a handful of basic primitives\n",
    "\n",
    "def tophalf(grid):\n",
    "    \"\"\" upper half \"\"\"\n",
    "    return grid[:len(grid) // 2]\n",
    "\n",
    "def bottomhalf(grid):\n",
    "    \"\"\"Extracts the lower half of the grid.\"\"\"\n",
    "    return grid[len(grid) // 2:]\n",
    "    \n",
    "def rot90(grid):\n",
    "    \"\"\" clockwise rotation by 90 degrees \"\"\"\n",
    "    return list(zip(*grid[::-1]))\n",
    "\n",
    "def rotate180(grid):\n",
    "    \"\"\"Rotates the grid 180 degrees.\"\"\"\n",
    "    return rot90(rot90(grid))\n",
    "\n",
    "def rotate270(grid):\n",
    "    \"\"\"Rotates the grid 270 degrees (counterclockwise by 90 degrees).\"\"\"\n",
    "    return rot90(rot90(rot90(grid)))\n",
    "    \n",
    "def leftrotate(grid):\n",
    "    \"\"\"Rotates the grid counterclockwise by 90 degrees.\"\"\"\n",
    "    return list(zip(*grid))[::-1]\n",
    "\n",
    "def hmirror(grid):\n",
    "    \"\"\" mirroring along horizontal \"\"\"\n",
    "    return grid[::-1]\n",
    "\n",
    "def vmirror(grid):\n",
    "    \"\"\"Mirroring along the vertical axis.\"\"\"\n",
    "    return [row[::-1] for row in grid]\n",
    "    \n",
    "def compress(grid):\n",
    "    \"\"\" removes frontiers \"\"\"\n",
    "    ri = [i for i, r in enumerate(grid) if len(set(r)) == 1]\n",
    "    ci = [j for j, c in enumerate(zip(*grid)) if len(set(c)) == 1]\n",
    "    return [[v for j, v in enumerate(r) if j not in ci] for i, r in enumerate(grid) if i not in ri]\n",
    "\n",
    "def trim(grid):\n",
    "    \"\"\" removes border \"\"\"\n",
    "    return [r[1:-1] for r in grid[1:-1]]\n",
    "\n",
    "def invert_colors(grid):\n",
    "    \"\"\"Inverts the colors of the grid (assuming binary grid of 0s and 1s).\"\"\"\n",
    "    return [[1 - cell for cell in row] for row in grid]\n",
    "\n",
    "# defining the DSL as the set of the primitives\n",
    "\n",
    "DSL_primitives = {tophalf, bottomhalf, rot90, leftrotate, hmirror, vmirror, compress, trim, rotate180, rotate270, invert_colors}\n",
    "primitive_names = {p.__name__ for p in DSL_primitives}\n",
    "print(f'DSL consists of {len(DSL_primitives)} primitives: {primitive_names}')\n",
    "# the maximum composition depth to consider\n",
    "MAX_DEPTH = 4\n",
    "\n",
    "def heuristic(grid):\n",
    "    \"\"\"Heuristic to prioritize transformations based on grid properties.\"\"\"\n",
    "    unique_colors = len(set(sum(grid, [])))\n",
    "    if unique_colors == 1:  # Uniform grid\n",
    "        return {trim, compress}\n",
    "    elif len(grid) == len(grid[0]):  # Square grid\n",
    "        return {rot90, rotate180, rotate270, hmirror, vmirror}\n",
    "    elif unique_colors > 3:  # More diverse colors, prioritize color-based transformations\n",
    "        return {invert_colors, hmirror, vmirror}\n",
    "    else:\n",
    "        return DSL_primitives\n",
    "# construct the program strings of all programs expressible by composing at most MAX_DEPTH primitives\n",
    "\n",
    "program_strings = []\n",
    "for depth in range(1, MAX_DEPTH+1):\n",
    "    primitive_tuples = itertools.product(*[primitive_names]*depth)\n",
    "    for primitives in primitive_tuples:\n",
    "        left_side = \"\".join([p + \"(\" for p in primitives])\n",
    "        right_side = ')' * depth\n",
    "        program_string = f'lambda grid: {left_side}grid{right_side}'\n",
    "        program_strings.append(program_string)\n",
    "\n",
    "\n",
    "# print some of the program strings\n",
    "print(f'Space to search consists of {len(program_strings)} programs:\\n')\n",
    "print('\\n'.join([*program_strings[:10], '...']))\n",
    "\n",
    "\n",
    "# map program strings to programs\n",
    "programs = {prog_str: eval(prog_str) for prog_str in program_strings}\n",
    "# for each task, search over the programs and if a working program is found, remember it\n",
    "\n",
    "guesses = dict()\n",
    "# iterate over all tasks\n",
    "for key, task in tqdm.tqdm(train_challenges.items()):\n",
    "    train_inputs = [example['input'] for example in task['train']]\n",
    "    train_outputs = [example['output'] for example in task['train']]\n",
    "    DSL_primitives = heuristic(train_inputs[0])\n",
    "    \n",
    "    hypotheses = []\n",
    "    # iterate over all programs\n",
    "    for program_string, program in programs.items():\n",
    "        try:\n",
    "            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n",
    "                # remember program if it explains all training examples\n",
    "                hypotheses.append(program_string)\n",
    "        except:\n",
    "            pass\n",
    "    # select first program for making predictions\n",
    "    if len(hypotheses) > 0:\n",
    "        print(f'found {len(hypotheses)} candidate programs for task {key}!')\n",
    "        guesses[key] = hypotheses[0]\n",
    "print(f'\\nMade guesses for {len(guesses)} tasks')\n",
    "# make predictions and evaluate them\n",
    "\n",
    "solved = dict()\n",
    "\n",
    "# iterate over all tasks for which a guess exists\n",
    "for key, program_string in guesses.items():\n",
    "    test_inputs = [example['input'] for example in train_challenges[key]['test']]\n",
    "    program = eval(program_string)\n",
    "    if all([program(i) == o for i, o in zip(test_inputs, train_solutions[key])]):\n",
    "        # mark predition as correct if all test examples are solved by the program\n",
    "        solved[key] = program_string\n",
    "\n",
    "\n",
    "print(f'Predictions correct for {len(solved)}/{len(guesses)} tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c69d6bf-5ec0-4a11-b43f-eafeb86bc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize solved tasks\n",
    "# def plot_task(task):\n",
    "#     \"\"\" plots a task \"\"\"\n",
    "#     examples = task['train']\n",
    "#     n_examples = len(examples)\n",
    "#     cmap = ListedColormap([\n",
    "#         '#000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n",
    "#         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'\n",
    "#     ])\n",
    "#     norm = Normalize(vmin=0, vmax=9)\n",
    "#     figure, axes = plt.subplots(2, n_examples, figsize=(n_examples * 4, 8))\n",
    "#     for column, example in enumerate(examples):\n",
    "#         axes[0, column].imshow(example['input'], cmap=cmap, norm=norm)\n",
    "#         axes[1, column].imshow(example['output'], cmap=cmap, norm=norm)\n",
    "#         axes[0, column].axis('off')\n",
    "#         axes[1, column].axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "# for key, program_string in solved.items():\n",
    "#     print(f'For task \"{key}\", found program \"{program_string}\"')\n",
    "#     plot_task(train_challenges[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f34083-882b-4fb9-b5fd-92dd75772dc3",
   "metadata": {},
   "source": [
    "# Generating a submission\n",
    "\n",
    "To generate a submission you need to output a file called `submission.json` that has the following format:\n",
    "\n",
    "```\n",
    "{\"00576224\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"009d5c81\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " \"12997ef3\": [{\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]},\n",
    "              {\"attempt_1\": [[0, 0], [0, 0]], \"attempt_2\": [[0, 0], [0, 0]]}],\n",
    " ...\n",
    "}\n",
    "```\n",
    "\n",
    "In this case, the task ids come from `test_challenges`. There may be multiple (i.e., >1) test items per task. Therefore, the dictionary has a list of dicts for each task. These submission dictionaries should appear in the same order as the test items from `test_challenges`. Additionally, you can provide two attempts for each test item. In fact, you **MUST** provide two attempts. If you only want to generate a single attempt, then just submit the same answer for both attempts (or submit an empty submission like the ones shown in the example snippit just above.\n",
    "\n",
    "Here is how we might create a blank submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e25a0fb-e206-4505-b597-8863c41c7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 400/400 [00:00<00:00, 7078.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 candidate programs for task 68b67ca3!\n",
      "\n",
      "Made guesses for 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an empty submission dict for output\n",
    "submission = {}\n",
    "counter = dict()\n",
    "# iterate over all tasks\n",
    "for key, task in tqdm.tqdm(test_challenges.items()):\n",
    "    train_inputs = [example['input'] for example in task['train']]\n",
    "    train_outputs = [example['output'] for example in task['train']]\n",
    "    DSL_primitives = heuristic(train_inputs[0])\n",
    "    hypotheses = []\n",
    "    # iterate over all programs\n",
    "    for program_string, program in programs.items():\n",
    "        try:\n",
    "            if all([program(i) == o for i, o in zip(train_inputs, train_outputs)]):\n",
    "                # remember program if it explains all training examples\n",
    "                hypotheses.append(program_string)\n",
    "        except:\n",
    "            pass\n",
    "    # select first program for making predictions\n",
    "    predictions = [example['input'] for example in task['test']]\n",
    "    if len(hypotheses) > 0:\n",
    "        print(f'found {len(hypotheses)} candidate programs for task {key}!')\n",
    "        program_string = hypotheses[0]\n",
    "        program = eval(program_string)\n",
    "        counter[key] = hypotheses[0]\n",
    "        try:\n",
    "            predictions = [program(example['input']) for example in task['test']]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    submission[key] = [{'attempt_1': grid, 'attempt_2': grid} for grid in predictions]\n",
    "print(f'\\nMade guesses for {len(counter)} tasks')\n",
    "    \n",
    "with open('submission.json', 'w') as fp:\n",
    "    json.dump(submission, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f7599",
   "metadata": {},
   "source": [
    "# Scoring Your Submission\n",
    "\n",
    "If you do not want to wait for gradescope to score your solution, we have provided the following code to score your submission. Note that the maximum possibe score is 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c39df0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_submission():\n",
    "    with open('../input/arc-prize-2024/arc-agi_evaluation_solutions.json', 'r') as sol_file:\n",
    "        solutions = json.load(sol_file)\n",
    "    \n",
    "    with open('submission.json', 'r') as sub_file:\n",
    "        submission = json.load(sub_file)\n",
    "    \n",
    "    overall_score = 0\n",
    "\n",
    "    for task in solutions:\n",
    "        score = 0\n",
    "        for i, answer in enumerate(solutions[task]):\n",
    "            attempt1_correct = submission[task][i]['attempt_1'] == answer\n",
    "            attempt2_correct = submission[task][i]['attempt_2'] == answer\n",
    "            score += int(attempt1_correct or attempt2_correct)\n",
    "\n",
    "        score /= len(solutions[task])\n",
    "\n",
    "        overall_score += score \n",
    "\n",
    "    print(overall_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6818bb2",
   "metadata": {},
   "source": [
    "You can run the above code by uncommenting the following code block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b131fe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "score_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8a5dc1a6-54fd-4f54-a4cf-cb1630eed78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_submission = dict()\n",
    "# iterate over all tasks for which a guess exists\n",
    "# for key, program_string in counter.items():\n",
    "#     submission_inputs = [example['input'] for example in test_challenges[key]['test']]\n",
    "#     program = program_string\n",
    "#     plot_task(test_challenges[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6466c66-c324-44c2-8e05-3860e0fe1fdd",
   "metadata": {},
   "source": [
    "# Confused about where to get started? \n",
    "\n",
    "If you're not sure what an initial solution might look like, then consider looking at public notebooks here: https://www.kaggle.com/competitions/arc-prize-2024/code or joining the public discussion here: https://www.kaggle.com/competitions/arc-prize-2024/discussion.\n",
    "\n",
    "One example notebook that uses a very simple knowledge-based approach is this one: https://www.kaggle.com/code/michaelhodel/program-synthesis-starter-notebook/notebook, which conducts search over a space of domain specific block languages to form hypotheses and then applies these to test items."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
